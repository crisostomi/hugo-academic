@article{10.1145/3519022,
 abstract = {Deep learning approaches have recently raised the bar in many fields, from Natural Language Processing to Computer Vision, by leveraging large amounts of data. However, they could fail when the retrieved information is not enough to fit the vast number of parameters, frequently resulting in overfitting and therefore in poor generalizability. Few-Shot Learning aims at designing models that can effectively operate in a scarce data regime, yielding learning strategies that only need few supervised examples to be trained. These procedures are of both practical and theoretical importance, as they are crucial for many real-life scenarios in which data is either costly or even impossible to retrieve. Moreover, they bridge the distance between current data-hungry models and human-like generalization capability. Computer vision offers various tasks that can be few-shot inherent, such as person re-identification. This survey, which to the best of our knowledge is the first tackling this problem, is focused on Few-Shot Object Detection, which has received far less attention compared to Few-Shot Classification due to the intrinsic challenge level. In this regard, this review presents an extensive description of the approaches that have been tested in the current literature, discussing their pros and cons, and classifying them according to a rigorous taxonomy.},
 address = {New York, NY, USA},
 articleno = {242},
 author = {Antonelli, Simone and Avola, Danilo and Cinque, Luigi and Crisostomi, Donato and Foresti, Gian Luca and Galasso, Fabio and Marini, Marco Raoul and Mecca, Alessio and Pannone, Daniele},
 doi = {10.1145/3519022},
 issn = {0360-0300},
 issue_date = {January 2022},
 journal = {ACM Comput. Surv.},
 keywords = {benchmarks and metrics for object detection, Deep learning for few-shot object detection, dataset for object detection},
 month = {sep},
 number = {11s},
 numpages = {37},
 publisher = {Association for Computing Machinery},
 title = {Few-Shot Object Detection: A Survey},
 url = {https://doi.org/10.1145/3519022},
 volume = {54},
 year = {2022}
}

